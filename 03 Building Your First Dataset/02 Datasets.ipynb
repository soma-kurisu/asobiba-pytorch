{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "\n",
    "import pandas as pd\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['mpg', 'cyl', 'disp', 'hp', 'weight', 'acc', 'year', 'origin'] \n",
    "\n",
    "df = pd.read_csv(url, names=column_names, na_values='?', comment='\\t', sep=' ', skipinitialspace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mpg'].values[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['mpg', 'hp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['mpg', 'hp']].values[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = (df['year'] == 70)\n",
    "df[cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation-Test Split: Shuffling\n",
    "\n",
    "df['year'].values[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled = df.sample(frac=1, random_state=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainval, test = train_test_split(shuffled, test_size=0.16, shuffle=False)\n",
    "train, val = train_test_split(trainval, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Data\n",
    "\n",
    "is_missing_attr = train.isna()\n",
    "n_missing_attr = is_missing_attr.sum(axis=1)\n",
    "train[n_missing_attr > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.dropna(inplace=True)\n",
    "test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous Attributes\n",
    "\n",
    "cont_attr = ['mpg', 'disp', 'hp', 'weight', 'acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train[cont_attr[1:]]\n",
    "train_features.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_means = train_features.mean()\n",
    "train_standard_deviations = train_features.std()\n",
    "train_means, train_standard_deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_standardized_features = (train_features - train_means)/train_standard_deviations\n",
    "train_standardized_features.mean(), train_standardized_features.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_standardized_features.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = val[cont_attr[1:]]\n",
    "val_standardized_features = (val_features - train_means)/train_standard_deviations\n",
    "val_standardized_features.mean(), val_standardized_features.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test[cont_attr[1:]]\n",
    "test_standardized_features = (test_features - train_means)/train_standard_deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_features.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.mean_, scaler.var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_data = {}\n",
    "standardized_data['train'] = scaler.transform(train_features)\n",
    "standardized_data['val'] = scaler.transform(val_features)\n",
    "standardized_data['test'] = scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "def standardize(df, cont_attr, scaler=None):\n",
    "    cont_X = df[cont_attr].values\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "    scaler.fit(cont_X)\n",
    "    cont_X = scaler.transform(cont_X)\n",
    "    cont_X = torch.as_tensor(cont_X, dtype=torch.float32)\n",
    "    return cont_X, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_data = {}\n",
    "# The training set is used to fit a scaler\n",
    "standardized_data['train'], scaler = standardize(train_features, cont_attr[1:])\n",
    "# The scaler is used as argument to the other datasets\n",
    "standardized_data['val'], _ = standardize(val_features, cont_attr[1:], scaler)\n",
    "standardized_data['test'], _ = standardize(test_features, cont_attr[1:], scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete and Categorical Attributes\n",
    "\n",
    "cyls = sorted(train['cyl'].unique())\n",
    "cyls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyls_map = dict((v, i) for i, v in enumerate(cyls))\n",
    "cyls_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 8\n",
    "lookup_table = torch.randn((len(cyls), n_dim))\n",
    "lookup_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = cyls_map[6]\n",
    "lookup_table[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "emb_table = nn.Embedding(len(cyls), n_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = cyls_map[6]\n",
    "emb_table(torch.as_tensor([idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_table = torch.eye(len(cyls))\n",
    "ohe_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = cyls_map[6]\n",
    "ohe_table[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "disc_attr = ['cyl', 'year', 'origin'] \n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "encoder.fit(train[disc_attr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat_features = encoder.transform(train[disc_attr])\n",
    "train_cat_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[disc_attr].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df, cat_attr, encoder=None):\n",
    "    cat_X = df[cat_attr].values\n",
    "    if encoder is None:\n",
    "        encoder = OrdinalEncoder()\n",
    "        encoder.fit(cat_X)\n",
    "    cat_X = encoder.transform(cat_X)\n",
    "    cat_X = torch.as_tensor(cat_X, dtype=torch.int)\n",
    "    return cat_X, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data = {}\n",
    "cat_data['train'], encoder = encode(train, disc_attr)\n",
    "cat_data['val'], _ = encode(val, disc_attr, encoder)\n",
    "cat_data['test'], _ = encode(test, disc_attr, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_table(cat_data['train'][:, 0]) # cylinders is the first (zero) column"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
